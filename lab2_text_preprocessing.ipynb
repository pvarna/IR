{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main steps for building an index:\n",
    "\n",
    "![Sort-Based-Index](img/index_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHurricane-force winds have struck central and northern Portugal, leaving 300,000 homes without power.',\n",
       " 'The remnants of Hurricane Leslie swept in overnight on Saturday, with winds gusting up to 176km/h (109mph).',\n",
       " 'Civil defence officials said 27 people suffered minor injuries, with localised flooding, hundreds of trees uprooted and a number of flights cancelled.',\n",
       " 'The storm, one of the most powerful to ever hit the country, is now passing over northern Spain.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "example_document = \"\"\"\n",
    "Hurricane-force winds have struck central and northern Portugal, leaving 300,000 homes without power.\n",
    "\n",
    "The remnants of Hurricane Leslie swept in overnight on Saturday, with winds gusting up to 176km/h (109mph).\n",
    "\n",
    "Civil defence officials said 27 people suffered minor injuries, with localised flooding, hundreds of trees uprooted and a number of flights cancelled.\n",
    "\n",
    "The storm, one of the most powerful to ever hit the country, is now passing over northern Spain.\n",
    "\"\"\"\n",
    "sentences = sent_tokenizer.tokenize(example_document)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace tokenizer:\n",
      "['Hurricane-force', 'winds', 'have', 'struck', 'central', 'and', 'northern', 'Portugal,', 'leaving', '300,000', 'homes', 'without', 'power.']\n",
      "\n",
      "Word Punctuation Tokenizer:\n",
      "['Civil', 'defence', 'officials', 'said', '27', 'people', 'suffered', 'minor', 'injuries', ',', 'with', 'localised', 'flooding', ',', 'hundreds', 'of', 'trees', 'uprooted', 'and', 'a', 'number', 'of', 'flights', 'cancelled', '.']\n",
      "['On', 'a', '$', '50', ',', '000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366', '.', '88', '.']\n",
      "\n",
      "Tree bank tokenizer:\n",
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', 'percent', ',', 'the', 'monthly', 'payment', 'would', 'be', '$', '366.88', '.']\n",
      "['My', 'email', 'is', 'admin', '@', 'sth.com']\n",
      "\n",
      "Tweeter Tokenizer:\n",
      "['My', 'email', 'is', 'admin@sth.com']\n",
      "['Last', 'sunny', 'day', 'before', 'winter', ':)', '#sunisenergy']\n",
      "['Here', 'is', 'my', 'ip', 'address', '192.33', '.', '24.45']\n",
      "['I', \"can't\", 'stop', 'singing', 'that', 'song', '!']\n",
      "['The', 'remnants', 'of', 'Hurricane', 'Leslie', 'swept', 'in', 'overnight', 'on', 'Saturday', ',', 'with', 'winds', 'gusting', 'up', 'to', '176km', '/', 'h', '(', '109mph', ')', '.']\n",
      "['Eugenia', 'e', 'Jack', 'sposi', 'e', 'innamorati', '(', 'ma', 'la', 'magia', 'di', 'Harry', 'e', 'Meghan', 'non', 'c', 'â€™', 'Ã¨', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer, WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "tweet_tok, whsp_tok, wordpunct_tok, treebank_tok = TweetTokenizer(), WhitespaceTokenizer(), WordPunctTokenizer(), TreebankWordTokenizer()\n",
    "\n",
    "print(\"Whitespace tokenizer:\")\n",
    "print(whsp_tok.tokenize(sentences[0]))\n",
    "\n",
    "print(\"\\nWord Punctuation Tokenizer:\")\n",
    "print(wordpunct_tok.tokenize(sentences[2]))\n",
    "print(wordpunct_tok.tokenize('On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.'))\n",
    "\n",
    "print(\"\\nTree bank tokenizer:\")\n",
    "print(treebank_tok.tokenize('On a $50,000 mortgage of 30 years at 8 percent, the monthly payment would be $366.88.'))\n",
    "print(treebank_tok.tokenize('My email is admin@sth.com'))\n",
    "\n",
    "print(\"\\nTweeter Tokenizer:\")\n",
    "print(tweet_tok.tokenize('My email is admin@sth.com'))\n",
    "print(tweet_tok.tokenize('Last sunny day before winter :) #sunisenergy'))\n",
    "print(tweet_tok.tokenize('Here is my ip address 192.33.24.45')) # we might want to fix this\n",
    "print(tweet_tok.tokenize(\"I can't stop singing that song!\")) # we might want to split can't in can, 't instead\n",
    "print(tweet_tok.tokenize(sentences[1])) # maybe we want to keep  km/h together apart from 176?\n",
    "print(tweet_tok.tokenize('Eugenia e Jack sposi e innamorati (ma la magia di Harry e Meghan non câ€™Ã¨)')) # maybe we want language specific tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: play with the tokenizers and find more vulnerabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Depending on the domain and the task at hand, we would like to make text normalization. Some examples of text normalizing steps can include:\n",
    "- Whole chunks of the document might be irrelevant, e.g. html tags.\n",
    "- __Numbers__ - we can remove them or replace them with a common token.\n",
    "- We can do the same for all other token types we don't care about, e.g. URLs, @ mentions, ðŸ˜€, etc.\n",
    "- We can __lower-case__ all words or even use a tool to upper-case all named entities like geographic names.\n",
    "- We might want to add __equivalence classes__ for matching synonyms, abbreviations, etc.\n",
    "- Remove __stopwords__ - the list of stopwords is usually manually curated\n",
    "- Google N-Grams - a huge corpus of statistics, showing frequency of word n-grams in web pages. It contains statistics for many languages and can be used to further examine frequently used words. \n",
    "- __Stemming__ - crude removal of prefixes and suffixes to reduce the word form to match words like car-sharing\n",
    "- __Lemmatiazation__ - replacing the word by its lemma\n",
    "\n",
    "__Exercise__: Find examples when we wouldn't like to normalize text with each of the proposed methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\didimitrov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\didimitrov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car Car car\n",
      "sharing sharing share\n",
      "will will will\n",
      "bring bring bring\n",
      "a a a\n",
      "whole whole whole\n",
      "new new new\n",
      "era era era\n",
      "to to to\n",
      "the the the\n",
      "automobile automobile automobil\n",
      "industry industry industri\n",
      ". . .\n",
      "Eventually Eventually eventu\n",
      ", , ,\n",
      "it it it\n",
      "may may may\n",
      "even even even\n",
      "decreas decreas decrea\n",
      "air air air\n",
      "pollutions pollution pollut\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens = tweet_tok.tokenize('Car sharing will bring a whole new era to the automobile industry. Eventually, it may even decreas air pollutions.')\n",
    "for token in tokens:\n",
    "    print(token, lemmatizer.lemmatize(token), stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\didimitrov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "#### Resource - https://www.geeksforgeeks.org/introduction-to-stemming/ / https://www.semanticscholar.org/paper/A-Comparative-Study-of-Stemming-Algorithms-Ms-.-Jivani/1c0c0fa35d4ff8a2f925eb955e48d655494bd167?p2df\n",
    "\n",
    "* Stemming is used in information retrieval systems like search engines.\n",
    "* It is used to determine domain vocabularies in domain analysis.\n",
    "* To display search results by indexing while documents are evolving into numbers and to map documents to common subjects by stemming.\n",
    "* Sentiment Analysis, which examines reviews and comments made by different users about anything, is frequently used for product analysis, such as for online retail stores. Before it is interpreted, stemming is accepted in the form of the text-preparation mean.\n",
    "* A method of group analysis used on textual materials is called document clustering (also known as text clustering). Important uses of it include subject extraction, automatic document structuring, and quick information retrieval.\n",
    "\n",
    "##### Popular stemmers:\n",
    "* **Porterâ€™s Stemmer** -  removing the commoner morphological and inflexional endings from words in English\n",
    "    * Advantage: It produces the best output as compared to other stemmers and it has less error rate.\n",
    "    * Limitation:  Morphological variants produced are not always real words.\n",
    "* **Lovins Stemmer** - removes the longest suffix from a word then the word is recorded to convert this stem into valid words\n",
    "    * Advantage: It is fast and handles irregular plurals like 'teeth' and 'tooth' etc.\n",
    "    * Limitation: It is time consuming and frequently fails to form words from stem.\n",
    "* **Dawson Stemmer** - suffixes are stored in the reversed order indexed by their length and last letter\n",
    "    * Advantage: It is fast in execution and covers more suffices.\n",
    "    * Limitation: It is very complex to implement.\n",
    "* **Krovetz Stemmer** - 1) Convert the plural form of a word to its singular form. 2) Convert the past tense of a word to its present tense and remove the suffix â€˜ingâ€™. \n",
    "    * Advantage: It is light in nature and can be used as pre-stemmer for other stemmers.\n",
    "    * Limitation: It is inefficient in case of large documents.\n",
    "* **Snowball Stemmer** - can map non-English words too. Also called Porter2 Stemmer (has performance improvements,  in addition to the multi-lingual application)\n",
    "* **N-Gram Stemmer**\n",
    "   * Advantage: It is based on string comparisons and it is language dependent.\n",
    "   * Limitation: It requires space to create and index the n-grams and it is not time efficient.\n",
    "* **and many others...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\didimitrov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "|    | original   | PorterStemmer   | LancasterStemmer   | SnowballStemmer   |\n",
      "|---:|:-----------|:----------------|:-------------------|:------------------|\n",
      "|  0 | agreed     | agre            | agree              | agre              |\n",
      "|  1 | children   | children        | childr             | children          |\n",
      "|  2 | flies      | fli             | fli                | fli               |\n",
      "|  3 | humbled    | humbl           | humbl              | humbl             |\n",
      "|  4 | colonizer  | colon           | colon              | colon             |\n",
      "|  5 | owned      | own             | own                | own               |\n",
      "|  6 | meeting    | meet            | meet               | meet              |\n",
      "|  7 | sitting    | sit             | sit                | sit               |\n",
      "|  8 | understood | understood      | understood         | understood        |\n",
      "|  9 | whom       | whom            | whom               | whom              |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate\n",
    "from nltk.stem import *\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "stem_list = [porter.PorterStemmer(), lancaster.LancasterStemmer(),snowball.SnowballStemmer('english')]\n",
    "#List of words\n",
    "examples = ['agreed', 'children','flies', 'humbled', 'colonizer', 'owned', 'meeting','sitting', 'understood', 'whom']\n",
    "result = defaultdict(list)\n",
    "for word in examples:\n",
    "    result['original'].append(word)\n",
    "    for stemmer in stem_list:\n",
    "        result[stemmer.__class__.__name__].append(stemmer.stem(word))\n",
    "print(pd.DataFrame(result).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: explore the dataset from 20 news groups and create another preprocessing method, which will clean the text of the news articles:\n",
    "- Select one of the tokenizers or try to write one of your own\n",
    "- Clean the headings and any common parts\n",
    "- Lowercase the text and see if you might want to remove any token types (e.g. URLs, you might also want to split URLs to meaningful words and use them as tokens)\n",
    "- Explore the most frequent words in the corpus and add more stop-words (depending on your tokenizer you might also need to modify the stopwords, too). Then, remove them from the text.\n",
    "- Have a method variable for choosing lemmatization or stemming\n",
    "- Finally compare how all the steps reduced/modified the index.\n",
    "\n",
    "\n",
    "You will also have to include all the steps for the query words in your query methods, too!\n",
    "- Explore how the quering results changed in both positive and negative ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from string import punctuation\n",
    "from os import scandir\n",
    "tw_tokenizer = TweetTokenizer()\n",
    "\n",
    "def tokenize_documents(path, tokenizer):\n",
    "    \"\"\"\n",
    "    Implement a tokenizer function that accepts a directory path and a tokenizer. \n",
    "    The function should return a list of tokenized documents. You can also return a dictionary with document names\n",
    "    \"\"\"\n",
    "   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['path',\n",
       " 'cantaloupe.srv.cs.cmu.edu',\n",
       " 'das-news.harvard.edu',\n",
       " 'ogicse',\n",
       " 'uwm.edu',\n",
       " 'zaphod.mps.ohio-state.edu',\n",
       " 'howland.reston.ans.net',\n",
       " 'sol.ctr.columbia.edu',\n",
       " 'news.cs.columbia.edu',\n",
       " 'ji',\n",
       " 'from',\n",
       " 'ji@cs.columbia.edu',\n",
       " 'john',\n",
       " 'ioannidis',\n",
       " 'newsgroups',\n",
       " 'sci.crypt',\n",
       " 'subject',\n",
       " 're',\n",
       " 'source',\n",
       " 'of',\n",
       " 'random',\n",
       " 'bits',\n",
       " 'on',\n",
       " 'a',\n",
       " 'unix',\n",
       " 'workstation',\n",
       " 'message-id',\n",
       " '<c5jp0k.4p5@cs.columbia.edu>',\n",
       " 'date',\n",
       " '15',\n",
       " 'apr',\n",
       " '93',\n",
       " '21:57',\n",
       " '55',\n",
       " 'gmt',\n",
       " 'article-i',\n",
       " 'd',\n",
       " 'cs',\n",
       " 'c5jp0k',\n",
       " '4p5',\n",
       " 'references',\n",
       " '<897@pivot.sbi.com>',\n",
       " '<c5ja6s.a59@cs.psu.edu>',\n",
       " 'sender',\n",
       " 'news@cs.columbia.edu',\n",
       " 'the',\n",
       " 'daily',\n",
       " 'news',\n",
       " 'organization',\n",
       " 'columbia',\n",
       " 'university',\n",
       " 'department',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'lines',\n",
       " '35',\n",
       " 'in',\n",
       " 'article',\n",
       " '<c5ja6s.a59@cs.psu.edu>',\n",
       " 'so@eiffel.cs.psu.edu',\n",
       " 'nicol',\n",
       " 'c',\n",
       " 'so',\n",
       " 'writes',\n",
       " 'in',\n",
       " 'article',\n",
       " '<897@pivot.sbi.com>',\n",
       " 'bet@sbi.com',\n",
       " 'bennett',\n",
       " 'todd',\n",
       " 'salomon',\n",
       " 'brothers',\n",
       " 'inc',\n",
       " 'ny',\n",
       " 'writes',\n",
       " 'this',\n",
       " 'came',\n",
       " 'up',\n",
       " 'because',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'up',\n",
       " 'mit-magic-cookie',\n",
       " '1',\n",
       " 'security',\n",
       " 'for',\n",
       " 'x11r5',\n",
       " 'for',\n",
       " 'this',\n",
       " 'to',\n",
       " 'work',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'stick',\n",
       " 'some',\n",
       " 'bits',\n",
       " 'that',\n",
       " 'an',\n",
       " 'intruder',\n",
       " \"can't\",\n",
       " 'guess',\n",
       " 'in',\n",
       " 'a',\n",
       " 'file',\n",
       " 'readable',\n",
       " 'only',\n",
       " 'by',\n",
       " 'you',\n",
       " 'which',\n",
       " 'x',\n",
       " 'client',\n",
       " 'applications',\n",
       " 'read',\n",
       " 'they',\n",
       " 'pass',\n",
       " 'the',\n",
       " 'bits',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'server',\n",
       " 'when',\n",
       " 'they',\n",
       " 'want',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'a',\n",
       " 'connection',\n",
       " '...',\n",
       " 'what',\n",
       " 'i',\n",
       " 'settled',\n",
       " 'on',\n",
       " 'was',\n",
       " 'grabbing',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'traffic',\n",
       " 'off',\n",
       " 'the',\n",
       " 'network',\n",
       " 'basically',\n",
       " 'i',\n",
       " 'ran',\n",
       " 'etherfind',\n",
       " 'u',\n",
       " 'x',\n",
       " 'greater',\n",
       " '0',\n",
       " 'compress',\n",
       " 'and',\n",
       " 'skipped',\n",
       " 'over',\n",
       " '10k',\n",
       " 'of',\n",
       " 'output',\n",
       " 'then',\n",
       " 'grabbed',\n",
       " 'my',\n",
       " 'bits',\n",
       " 'as',\n",
       " 'best',\n",
       " 'i',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'these',\n",
       " 'are',\n",
       " 'bits',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'be',\n",
       " 'reasonably',\n",
       " 'uniformly',\n",
       " 'distributed',\n",
       " 'and',\n",
       " 'quite',\n",
       " 'unguessable',\n",
       " 'by',\n",
       " 'an',\n",
       " 'intruder',\n",
       " 'for',\n",
       " 'your',\n",
       " 'application',\n",
       " 'what',\n",
       " 'you',\n",
       " 'can',\n",
       " 'do',\n",
       " 'is',\n",
       " 'to',\n",
       " 'encrypt',\n",
       " 'the',\n",
       " 'real-time',\n",
       " 'clock',\n",
       " 'value',\n",
       " 'with',\n",
       " 'a',\n",
       " 'secret',\n",
       " 'key',\n",
       " 'for',\n",
       " 'a',\n",
       " 'good',\n",
       " 'discussion',\n",
       " 'of',\n",
       " 'cryptographically',\n",
       " 'good',\n",
       " 'random',\n",
       " 'number',\n",
       " 'generators',\n",
       " 'check',\n",
       " 'out',\n",
       " 'the',\n",
       " 'draft-ietf-security-randomness-00.txt',\n",
       " 'internet',\n",
       " 'draft',\n",
       " 'available',\n",
       " 'at',\n",
       " 'your',\n",
       " 'local',\n",
       " 'friendly',\n",
       " 'internet',\n",
       " 'drafts',\n",
       " 'repository',\n",
       " 'a',\n",
       " 'reasonably',\n",
       " 'source',\n",
       " 'of',\n",
       " 'randomness',\n",
       " 'is',\n",
       " 'the',\n",
       " 'output',\n",
       " 'of',\n",
       " 'a',\n",
       " 'cryptographic',\n",
       " 'hash',\n",
       " 'function',\n",
       " 'e',\n",
       " 'g',\n",
       " 'md5',\n",
       " 'when',\n",
       " 'fed',\n",
       " 'with',\n",
       " 'a',\n",
       " 'large',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'more-or-less',\n",
       " 'random',\n",
       " 'data',\n",
       " 'for',\n",
       " 'example',\n",
       " 'running',\n",
       " 'md5',\n",
       " 'on',\n",
       " 'dev',\n",
       " 'mem',\n",
       " 'is',\n",
       " 'a',\n",
       " 'slow',\n",
       " 'but',\n",
       " 'random',\n",
       " 'enough',\n",
       " 'source',\n",
       " 'of',\n",
       " 'random',\n",
       " 'bits',\n",
       " 'there',\n",
       " 'are',\n",
       " 'bound',\n",
       " 'to',\n",
       " 'be',\n",
       " '128',\n",
       " 'bits',\n",
       " 'of',\n",
       " 'entropy',\n",
       " 'in',\n",
       " 'the',\n",
       " 'tens',\n",
       " 'or',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'megabytes',\n",
       " 'of',\n",
       " 'data',\n",
       " 'in',\n",
       " 'a',\n",
       " 'modern',\n",
       " \"workstation's\",\n",
       " 'memory',\n",
       " 'as',\n",
       " 'a',\n",
       " 'fair',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'them',\n",
       " 'are',\n",
       " 'system',\n",
       " 'timers',\n",
       " 'i',\n",
       " 'o',\n",
       " 'buffers',\n",
       " 'etc',\n",
       " 'ji']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_documents('data/mini_newsgroups/sci.crypt/',tw_tokenizer)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>Observing how the most frequent tokens change:\n",
      ">>>>>Original Tokens:\n",
      "    text  freq\n",
      "52   the  1718\n",
      "45    to   831\n",
      "19    of   711\n",
      "88     a   563\n",
      "173  and   506\n",
      ">>>>>>Removed stopwords:\n",
      "                          text  freq\n",
      "15                   sci.crypt   158\n",
      "1    cantaloupe.srv.cs.cmu.edu   150\n",
      "271                 encryption   148\n",
      "21                         key   133\n",
      "269                    clipper   115\n",
      ">>>>>>Applied stemming:\n",
      "            text  freq     stemmed\n",
      "3036   directive     1      direct\n",
      "3037  additional     1       addit\n",
      "3038         mat     1         mat\n",
      "3039      heyman     1      heyman\n",
      "5768  0)794-3017     1  0)794-3017\n",
      ">>>>>>Merge same stemmed tokens into one:\n",
      "stemmed\n",
      "key                          198\n",
      "encrypt                      187\n",
      "use                          185\n",
      "sci.crypt                    158\n",
      "cantaloupe.srv.cs.cmu.edu    150\n",
      "Name: freq, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\">>>>>>Observing how the most frequent tokens change:\")\n",
    "# normal list of words\n",
    "word_freqs = Counter(sum(tokenize_documents('data/mini_newsgroups/sci.crypt/',tw_tokenizer), []))\n",
    "df = DataFrame(list(word_freqs.items()), columns=['text', 'freq'])\n",
    "df.sort_values(['freq'], inplace=True, ascending=False)\n",
    "print(\">>>>>Original Tokens:\")\n",
    "print(df.head())\n",
    "\n",
    "# remove stopwords\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "df_no_stopwords = df.copy()\n",
    "df_no_stopwords = df_no_stopwords[~df_no_stopwords['text'].isin(en_stopwords)]\n",
    "print(\">>>>>>Removed stopwords:\")\n",
    "print(df_no_stopwords.head())\n",
    "\n",
    "# stem words\n",
    "stem = stemmer.stem\n",
    "df_no_stopwords['stemmed'] = df_no_stopwords['text'].apply(stem)\n",
    "print(\">>>>>>Applied stemming:\")\n",
    "print(df_no_stopwords.tail())\n",
    "\n",
    "# merge same stems into one row\n",
    "stemmed_freqs = df_no_stopwords.groupby(['stemmed'])['freq'].sum()\n",
    "stemmed_freqs.sort_values(inplace=True, ascending=False)\n",
    "print(\">>>>>>Merge same stemmed tokens into one:\")\n",
    "print(stemmed_freqs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
