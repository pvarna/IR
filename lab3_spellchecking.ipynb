{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description and pre-requisites:\n",
    "## Should have a list of words at hand\n",
    "- Some public list of all words in a language\n",
    "    - Google NGrams - http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
    "    - Project Gutenberg https://www.gutenberg.org/catalog/\n",
    "- Words encountered in indexed documents (can also contain errors)\n",
    "    - Manually add domain specific words\n",
    "    \n",
    "## Finding the correct word - Edit Distance\n",
    "- Measure the number of single-character edits required to change one word into another\n",
    "- Edits are: insert, delete, replace\n",
    "- Solved with dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distance Algorithm:<br>\n",
    "1. Initialize the matrix with zeros\n",
    "2. Initialize 1st row and column with numbers from 0 to N and  from 0 to M\n",
    "    * this is the cost of inserting a letter, when starting from the empty string\n",
    "3. The value of each of the remaining cells is: <br>\n",
    "<b>m[i, j] = min(<br>\n",
    "            m[i-1,j] + 1, # insert in s1\n",
    "            m[i,j-1] + 1, # insert in s2\n",
    "            m[i-1,j-1] + (0 if s1[i]==s2[j] else 1) # replace\n",
    ")</b> <br>\n",
    "4. The min edit distance is at m[-1,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| |S|A|T|U|R|D|A|Y|\n",
    "-|-|-|-|-|-|-|-|-|-|\n",
    " |0|1|2|3|4|5|6|7|8|\n",
    "S|1|0|1|2|3|4|5|6|7|\n",
    "U|2|1|1|2|2|3|4|5|6|\n",
    "N|3|2|2|2|3|3|4|5|6|\n",
    "D|4|3|3|3|3|4|3|4|5|\n",
    "A|5|4|3|4|4|4|4|3|4|\n",
    "Y|6|5|4|4|5|5|5|4|3|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.__ : Compute the edit distance between the words elephant and relevant. <br/>\n",
    "__Exercise 2.__ : Implement the minimum edit distance algorithm.\n",
    "\n",
    "Some corpora of misspellings can be found here: https://www.dcs.bbk.ac.uk/~ROGER/corpora.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    \"\"\"\n",
    "    Return the minumum edit distance between the two words\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " array([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       "        [1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
       "        [2, 1, 1, 2, 2, 3, 4, 5, 6],\n",
       "        [3, 2, 2, 2, 3, 3, 4, 5, 6],\n",
       "        [4, 3, 3, 3, 3, 4, 3, 4, 5],\n",
       "        [5, 4, 3, 4, 4, 4, 4, 3, 4],\n",
       "        [6, 5, 4, 4, 5, 5, 5, 4, 3]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('sunday', 'saturday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " array([[0, 1, 2, 3, 4, 5, 6],\n",
       "        [1, 0, 1, 2, 3, 4, 5],\n",
       "        [2, 1, 1, 2, 3, 3, 4],\n",
       "        [3, 2, 2, 2, 3, 4, 4],\n",
       "        [4, 3, 2, 3, 3, 4, 5],\n",
       "        [5, 4, 3, 3, 4, 4, 5],\n",
       "        [6, 5, 4, 4, 3, 4, 5],\n",
       "        [7, 6, 5, 5, 4, 3, 4],\n",
       "        [8, 7, 6, 6, 5, 4, 3]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('saturday', 'sunday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, array([[0, 1, 2, 3],\n",
      "       [1, 1, 2, 3],\n",
      "       [2, 2, 2, 3],\n",
      "       [3, 3, 3, 3]]))\n"
     ]
    }
   ],
   "source": [
    "print(edit_distance('elephant', 'relevant'))\n",
    "print(edit_distance( 'relevant','elephant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
       "        [1, 0, 1, 2, 3, 4, 5, 6, 7],\n",
       "        [2, 1, 1, 2, 3, 4, 5, 6, 7],\n",
       "        [3, 2, 2, 1, 2, 3, 4, 5, 6],\n",
       "        [4, 3, 3, 2, 1, 2, 3, 4, 5],\n",
       "        [5, 4, 4, 3, 2, 2, 3, 4, 5],\n",
       "        [6, 5, 5, 4, 3, 3, 3, 4, 5],\n",
       "        [7, 6, 6, 5, 4, 4, 4, 4, 5]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('lullaby', 'lollipop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output: <br>\n",
    "edit_distance('lullaby', 'lollipop') <br>\n",
    "> 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants:\n",
    "- Weighted Edit Distance\n",
    "- Keeping a backtrack of the symbol alignments\n",
    "- If a backtrack is not needed we can use only two matrix rows (memory reduced from O(nm) to O(2m))\n",
    "\n",
    "## Some implementations:\n",
    "- Levenstein Edit distance algorithm for Python in Cython for high performance \n",
    "    - https://github.com/gfairchild/pyxDamerauLevenshtein\n",
    "- Peter Norvig's implementation\n",
    "    - Idea : generate all possible words at edit distance 1 and 2, select those that are most probable (using a language model and priority of edit distance)\n",
    "    - Can incorporate error model (some data with common errors http://aspell.net/test/)\n",
    "    - http://norvig.com/spell-correct.html\n",
    "- Library TextBlob - contains code for training a Spellchecker from text\n",
    "    - https://github.com/sloria/TextBlob/blob/14f22102251ce1f02e8bcb3e74f86c037e3df822/textblob/_text.py#L1322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I have good spelling!\")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"I havv goood speling!\")\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyxdameraulevenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!pip3 install pyxDamerauLevenshtein\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyxdameraulevenshtein\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(damerau_levenshtein_distance(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmtih\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmith\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(normalized_damerau_levenshtein_distance(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmtih\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmith\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyxdameraulevenshtein'"
     ]
    }
   ],
   "source": [
    "#!pip3 install pyxDamerauLevenshtein\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "print(damerau_levenshtein_distance('smtih', 'smith'))\n",
    "print(normalized_damerau_levenshtein_distance('smtih', 'smith'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Problem with the Edit Distance approach:\n",
    "- Do we need to compute the edit distance to all dictionary terms? - __slow__\n",
    "- Solution 1: \n",
    "    - build __charcter n-gram index__ - n-gram:word\n",
    "        - Example: av -> avocado-> pavement, etc.\n",
    "    - retrieve the postings of each n-gram in the query\n",
    "    - merge postings, getting the words with most n-gram overlap\n",
    "    - car can get corrected to carbon, then to normalize the metric for overlap we use __Jaccard coefficient__:\n",
    "        - $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ \n",
    "        - If more than a threshold, then the words match\n",
    "- Solution 2:\n",
    "    - having to correct the word lates we will possibly compute edit distance to the words late, latest, latte\n",
    "    - for each variant we will compute the edit distance of the common prefix late\n",
    "    - rather use __tries__: store the whole vocabulary in a large trie, where one node is a symbol and the branches are the possible symbols following it\n",
    "    - __compute the edit distance just once for the same prefix__!\n",
    "    - Reference: http://stevehanov.ca/blog/index.php?id=114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.__ Implement the n-gram overlap spell-checker. Find the best Jaccard coefficient for the chosen dataset. <br>\n",
    "__Exercise 4.__ If you already decided what will be your project, build and tune your own spellchecker (or even one for Bulgarian!):\n",
    "- Choose and implementation and a list of words (Textblob's spellchecker is pretrained on Gutenberg's data)\n",
    "- Run your corpus through the spell-checker and find word, which were corrected, but are spelled correctly\n",
    "- Add them to the corpus of words you chose\n",
    "\n",
    "__Exercise 5.__ We have 2 sets A and B. They have 50 overlapping words. Size of the combined sets is 100. What would the jaccard coefficient be ? Would it change if we reduced set A by 10 words and increase set B by 10, but keep the 50 overlapping words? Review Overlap Coefficient. How will the results of this exercise differ compared to Jaccard Distance/Coeff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from string import punctuation\n",
    "from os import scandir\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def preprocess_documents(path):\n",
    "    \"\"\"\n",
    "    Returns a list of tokens for a document's content. \n",
    "    Tokens should not contain punctuation and should be lower-cased.\n",
    "    \"\"\"\n",
    "    tokenized_documents = []\n",
    "    for doc_f in scandir(path):\n",
    "        if not doc_f.is_file():\n",
    "            continue\n",
    "        content = open(doc_f).read()\n",
    "        sentences = sent_tokenize(content)\n",
    "        tokens = []\n",
    "        for _sent in sentences:\n",
    "            sent_tokens = tokenizer.tokenize(_sent)\n",
    "            sent_tokens = [_tok.lower() for _tok in sent_tokens if _tok not in punctuation]\n",
    "            tokens += sent_tokens\n",
    "        tokenized_documents.append(tokens)\n",
    "    return tokenized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['path',\n",
       " 'cantaloupe.srv.cs.cmu.edu',\n",
       " 'das-news.harvard.edu',\n",
       " 'ogicse',\n",
       " 'uwm.edu',\n",
       " 'wupost',\n",
       " 'uunet',\n",
       " 'world',\n",
       " 'edwards',\n",
       " 'from']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_documents = preprocess_documents('data/mini_newsgroups/rec.autos/')\n",
    "tokenized_documents[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_ngrams(word, n):\n",
    "    return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "        \n",
    "        \n",
    "def build_ngram_postings(n, tokenized_documents):\n",
    "    \"\"\"\n",
    "    Create postings from the tokenized documents - \n",
    "    For each n-gram we want to have the list of words, containing that n-gram. (E.g. av: [avocado, pavement, ...])\n",
    "    \"\"\"\n",
    "   pass\n",
    "\n",
    "def ngram_spellcheck(word, postings, n, jaccard_threshold=0.7):\n",
    "    \"\"\"Return the word from the postings structure, which is closest to the input word.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14285714285714285"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "w1 = set('mapping')\n",
    "w2 = set('mappings')\n",
    "\n",
    "nltk.jaccard_distance(w1, w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postings of bi-gram 'ar':\n",
      "['das-news.harvard.edu', 'das-news.harvard.edu', 'edwards', 'edwards@world.std.com', 'article-i', 'article', 'are', \"aren't\", 'year', 'rare']\n",
      "Closest words to enginering:\n",
      "[('engineering', 0.7647058823529411), ('enginerring', 0.7647058823529411)]\n"
     ]
    }
   ],
   "source": [
    "postings = build_ngram_postings(2, tokenized_documents)\n",
    "print(\"Postings of bi-gram 'ar':\")\n",
    "print(postings['ar'][:10])\n",
    "print(\"Closest words to enginering:\")\n",
    "print(ngram_spellcheck('enginering', postings, 2, jaccard_threshold=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example output: __<br>\n",
    "postings = build_ngram_postings(2, tokenized_documents) <br>\n",
    "print(\"Postings of bi-gram 'ar':\")<br>\n",
    "print(postings['ar'][:10])<br>\n",
    "print(\"Closest words to enginering:\")<br>\n",
    "print(ngram_spellcheck('enginering', postings, 2, jaccard_threshold=0.7))<br>\n",
    "\n",
    ">Postings of bi-gram 'ar':<br>\n",
    ">['article', 'darren', 'cars', 'hard', 'yard', 'compartment', 'are', 'early', 'area', 'antiauthoritarian']<br>\n",
    ">Closest words to enginering:<br>\n",
    ">['engineering', 'enginerring']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Building a Grammarly-like spelling/grammar correction for Bulgarian can be a good course project. \n",
    "If anyone is interested, we can discuss how to collect data and how to approach the task._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
