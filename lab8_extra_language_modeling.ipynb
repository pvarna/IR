{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "- A probabilistic technique used to __predict the probability that a sequence of tokens belongs to a language__!\n",
    "- __Applications__ - when having several options and have to __choose the most likely__, based on the context words:\n",
    "    - Compare sentences to tell __which are the 'good' ones__.\n",
    "    - __Spell-checking__ - we receive a user's query 'I want some cugar in my coffee.' -> cougar or sugar?\n",
    "    - __Automatic Speech recognition__ - get most likely sequence of words.\n",
    "    - __Machine Translation, Question Answering, Summarization__, etc.\n",
    "    - __Information retrieval__ - the document is likely to best answer a query if it is most likely to generate the query (it contains the same words as in the query)\n",
    "- Language models can __score and sort__ sentences. <br>\n",
    "P(I like apples) >> P(I lick apples) <br>\n",
    "- Language models encode some __grammaticality__. <br>\n",
    "P(I was going to call) >> P(I will going to call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Probabilistic Approach\n",
    "\n",
    "P(I, want, some, sugar, in, my, coffee) = ?\n",
    "\n",
    "## First shot: Chain Rule of probability\n",
    "\n",
    "$P(w_{1},..,w_{n}) = \\prod{P(w_{i}|w_{1},..,w_{i-1})}$\n",
    "\n",
    "P(I,want,some,cougar,in,my,coffee) = P(want|I)\\*P(some|I, want)\\*P(cougar|I,want,some)...\\*P(coffee|I,want,some,cougar,in,my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install tqdm\n",
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event', 'had', 'every', 'promise', 'of', 'happiness', 'for', 'her', 'friend', '.', 'Mr', '.', 'Weston', 'was', 'a', 'man', 'of', 'unexceptionable', 'character', ',', 'easy', 'fortune', ',', 'suitable', 'age', ',', 'and', 'pleasant', 'manners', ';', 'and', 'there', 'was', 'some', 'satisfaction', 'in', 'considering', 'with', 'what', 'self', '-', 'denying', ',', 'generous', 'friendship', 'she', 'had', 'always', 'wished', 'and', 'promoted', 'the', 'match', ';', 'but', 'it', 'was', 'a', 'black', 'morning', \"'\", 's', 'work', 'for', 'her', '.', 'The', 'want', 'of', 'Miss']\n"
     ]
    }
   ],
   "source": [
    "emma_words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "print(emma_words[450:520])\n",
    "emma_words_joined = \" \".join(emma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurence of: ['She'] 570\n",
      "Occurence of: ['She', 'was'] 110\n",
      "Occurence of: ['She', 'was'] 110\n",
      "Occurence of: ['She', 'was', 'the'] 6\n",
      "Occurence of: ['She', 'was', 'the'] 6\n",
      "Occurence of: ['She', 'was', 'the', 'youngest'] 1\n",
      "Occurence of: ['She', 'was', 'the', 'youngest'] 1\n",
      "Occurence of: ['She', 'was', 'the', 'youngest'] 1\n",
      "Sentence probability: 0.0017543859649122805\n",
      "Occurence of: ['I'] 4016\n",
      "Occurence of: ['I', 'want'] 18\n",
      "Occurence of: ['I', 'want'] 18\n",
      "Occurence of: ['I', 'want', 'some'] 0\n",
      "Occurence of: ['I', 'want', 'some'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in', 'my'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in', 'my'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in', 'my', 'coffee'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in', 'my', 'coffee'] 0\n",
      "Occurence of: ['I', 'want', 'some', 'sugar', 'in', 'my', 'coffee'] 0\n",
      "Sentence probability: 0.004482071713147411\n",
      "Occurence of: ['she'] 2023\n",
      "Occurence of: ['she', 'had'] 254\n",
      "Occurence of: ['she', 'had'] 254\n",
      "Occurence of: ['she', 'had', 'always'] 5\n",
      "Occurence of: ['she', 'had', 'always'] 5\n",
      "Occurence of: ['she', 'had', 'always', 'wished'] 1\n",
      "Occurence of: ['she', 'had', 'always', 'wished'] 1\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for'] 0\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for'] 0\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for', 'this'] 0\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for', 'this'] 0\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for', 'this', 'opportunity'] 0\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for', 'this', 'opportunity'] 0\n",
      "Occurence of: ['she', 'had', 'always', 'wished', 'for', 'this', 'opportunity'] 0\n",
      "Sentence probability: 0.0004943153732081068\n"
     ]
    }
   ],
   "source": [
    "sentences = [['She', 'was', 'the', 'youngest'],\n",
    "            ['I', 'want', 'some', 'sugar', 'in', 'my', 'coffee'],\n",
    "            ['she', 'had', 'always', 'wished', 'for', 'this', 'opportunity']]\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_prob = 1\n",
    "    for i in range(len(sentence)):\n",
    "        \n",
    "        context_freq = emma_words_joined.count(\" \".join(sentence[:i+1]))\n",
    "        print(\"Occurence of:\", sentence[:i+1], context_freq)\n",
    "        word_context_freq = emma_words_joined.count(\" \".join(sentence[:i+2]))\n",
    "        print(\"Occurence of:\", sentence[:i+2], word_context_freq)\n",
    "        \n",
    "        if context_freq == 0 or word_context_freq == 0: continue\n",
    "        sentence_prob *= word_context_freq / context_freq\n",
    "        \n",
    "    print(\"Sentence probability: {}\".format(sentence_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Problem: __ Will need a __big corpora of text__ in order to have all possible sentences, occurring enough times! But the possible sentences are too much and we will never see enough of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Simplifying Assumption of Independence:\n",
    "- Take only __l__ of the words before the given one:\n",
    "$ P(w_1,...,w_n) = \\prod{P(w_i|w_{i-l},...,w_{i-1})} $\n",
    "- __L=1 Unigram Model__:<br>\n",
    "$ P(w_i|w_{i-l},...,w_{i-1}) = P(w_i) $  <br>\n",
    "P(coffee|I,want,some,cougar,in,my) ~ P(coffee)\n",
    "- __L=2 Bigram Model__ : Condition only on the previous word<br>\n",
    "$ P(w_i|w_{i-l},...,w_{i-1}) = P(w_i|w_{i-1}) $ <br>\n",
    "P(coffee|I,want,some,cougar,in,my) ~ P(coffee|my)\n",
    "- If we want to add more context and thus, more dependence b/n words, we can make a tri-gram, a four-gram, etc. model!\n",
    "\n",
    "## Maximum Likelihood estimation: (the bigram case)\n",
    "$P(w_i|w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: What is the downside of using an unigram model? <br>\n",
    "<br>\n",
    "__Exercise__: Having this __training data__:<br>\n",
    "[s] I am Sam [/s]<br>\n",
    "[s] Sam I am [/s]<br>\n",
    "[s] Sam I like [/s]<br>\n",
    "[s] Sam I do like [/s]<br>\n",
    "[s] do I like Sam [/s]<br>\n",
    "\n",
    "Construct a __bigram model__ and then estimate:<br>\n",
    "1. What is the __most probable next word__ for the following word sequences:<br>\n",
    "(1) [s] Sam . . .<br>\n",
    "(2) [s] Sam I do . . .<br>\n",
    "(3) [s] Sam I am Sam . . . <br>\n",
    "(4) [s] do I like . . .<br>\n",
    "\n",
    "2. Which of the following sentences is __better__:<br>\n",
    "(5) [s] Sam I do I like [/s]<br>\n",
    "(6) [s] Sam I am [/s]<br>\n",
    "(7) [s] I do like Sam I am [/s]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent words:  [(',', 11454), ('.', 6928), ('to', 5183), ('the', 4844), ('and', 4672), ('of', 4279), ('I', 3178), ('a', 3004), ('was', 2385), ('her', 2381), (';', 2199), ('it', 2128), ('in', 2118), ('not', 2101), ('\"', 2004), ('be', 1970), ('she', 1778), ('that', 1730), ('you', 1677), ('had', 1606), ('as', 1387), ('--', 1382), ('he', 1365), ('for', 1321), ('have', 1301), ('is', 1220), ('with', 1187), ('Mr', 1153), ('very', 1151), ('but', 1148), ('.\"', 1138), ('his', 1088), (\"'\", 1007), ('at', 997), ('s', 933), ('so', 924), ('Emma', 865), ('all', 835), ('could', 825), ('would', 815), ('been', 759), ('him', 758), ('Mrs', 699), ('.--', 685), ('on', 677), ('any', 651), ('my', 619), ('no', 616), ('Miss', 592), ('were', 591), ('do', 580), ('-', 574), ('must', 564), ('me', 564), ('She', 562), ('will', 559), ('by', 558), ('which', 552), ('!', 549), ('from', 535), ('Harriet', 506), ('or', 490), ('said', 484), ('much', 478), ('more', 464), ('an', 452), ('are', 447), ('He', 441), ('such', 440), ('Weston', 439), ('what', 434), ('them', 432), ('am', 422), (',\"', 421), ('there', 420), ('this', 418), ('than', 415), ('one', 413), ('It', 400), ('every', 398), ('thing', 398), ('they', 392), ('Knightley', 389), ('Elton', 385), ('think', 380), ('if', 375), ('should', 366), ('The', 357), ('being', 356), ('little', 354), ('never', 347), ('good', 340), ('!--', 338), ('know', 337), ('your', 337), ('did', 335), ('only', 327), ('might', 322), ('well', 315), ('Woodhouse', 313), ('when', 312), ('say', 308), ('You', 303), ('Jane', 301), ('own', 301), ('their', 297), ('But', 293), ('who', 281), ('herself', 279), ('now', 273), ('time', 272), ('can', 270), ('quite', 269), ('how', 263), ('great', 263), ('we', 260), ('too', 253), ('some', 248), ('about', 246), ('most', 243), ('before', 243), ('has', 243), ('Fairfax', 241), ('nothing', 237), ('always', 235), ('man', 233), ('?\"', 230), ('thought', 226), ('And', 224), ('Churchill', 223), ('soon', 221), ('other', 220), ('see', 220), ('again', 219), ('dear', 217), ('may', 213), ('shall', 212), ('out', 212), ('without', 211), ('first', 209), ('Frank', 208), ('father', 207), ('sure', 204), ('?--', 200), ('made', 199), ('like', 199), ('body', 193), ('day', 190), ('up', 190), ('young', 190), ('ever', 189), ('Oh', 185), ('indeed', 181), ('friend', 177), ('?', 174), ('two', 171), ('though', 169), ('better', 166), ('into', 163), ('Hartfield', 160), ('just', 159), ('come', 159), ('give', 157), ('way', 155), ('really', 153), ('make', 152), ('then', 150), ('They', 148), ('Bates', 148), ('himself', 146), ('having', 145), ('rather', 145), ('us', 145), ('long', 144), ('!\"', 144), ('hope', 143), ('done', 142), ('seemed', 141), ('after', 140), ('over', 139), ('away', 138), ('cannot', 138), ('wish', 134), ('many', 133), ('here', 133), (':', 133), ('upon', 133), ('home', 130), ('woman', 129), ('go', 129), ('There', 129), ('enough', 129), ('mind', 128), ('No', 126), ('A', 125), ('Highbury', 125), ('does', 125), (';--', 124), ('happy', 122), ('even', 122)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:59<00:00,  3.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unigram_count</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11454</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6928</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5183</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4844</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4672</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unigram_count word\n",
       "0          11454    ,\n",
       "1           6928    .\n",
       "2           5183   to\n",
       "3           4844  the\n",
       "4           4672  and"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute co-occurence of bi-grams, then normalize by unigram frequency\n",
    "first_most_freq_words = collections.Counter(emma_words).most_common(200)\n",
    "print(\"Most frequent words: \", first_most_freq_words)\n",
    "\n",
    "unique_words = [_w[0] for _w in first_most_freq_words]\n",
    "unigram_counts = [emma_words.count(_w) for _w in tqdm(unique_words)]\n",
    "\n",
    "coocurrence_df = pd.DataFrame({'word': unique_words, 'unigram_count': unigram_counts})\n",
    "coocurrence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:28<00:00,  7.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unigram_count</th>\n",
       "      <th>word</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>I</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>There</th>\n",
       "      <th>enough</th>\n",
       "      <th>mind</th>\n",
       "      <th>No</th>\n",
       "      <th>A</th>\n",
       "      <th>Highbury</th>\n",
       "      <th>does</th>\n",
       "      <th>;--</th>\n",
       "      <th>happy</th>\n",
       "      <th>even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11454</td>\n",
       "      <td>,</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>387</td>\n",
       "      <td>1880</td>\n",
       "      <td>110</td>\n",
       "      <td>570</td>\n",
       "      <td>2547</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6928</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>977</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>292</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5183</td>\n",
       "      <td>to</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>356</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4844</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>179</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4672</td>\n",
       "      <td>and</td>\n",
       "      <td>144</td>\n",
       "      <td>33</td>\n",
       "      <td>66</td>\n",
       "      <td>376</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>165</td>\n",
       "      <td>397</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unigram_count word    ,   .   to  the   and   of    I     a  ...   There  \\\n",
       "0          11454    ,    0   0  289  387  1880  110  570  2547  ...       0   \n",
       "1           6928    .    0  20    0    0    52    0  977    54  ...      80   \n",
       "2           5183   to   26  21   10  339     4    5   16   356  ...       0   \n",
       "3           4844  the    0   2    9    1     0   10    2   179  ...       0   \n",
       "4           4672  and  144  33   66  376     6   42  165   397  ...       0   \n",
       "\n",
       "   enough  mind  No    A  Highbury  does  ;--  happy  even  \n",
       "0       1     0   0    3         2     5    0      2    17  \n",
       "1       0     0  83  292         2     0    0      0     0  \n",
       "2       0     2   0    6        21     0    0      0     0  \n",
       "3       0     4   0   15         4     0    0      2    47  \n",
       "4       0     4   1    2         5     2    1      8     8  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add bi-gram counts!\n",
    "for word in tqdm(unique_words):\n",
    "    coocurrence_df[word] = coocurrence_df.apply(lambda x: emma_words_joined.count(x['word']+\" \"+word), axis=1)\n",
    "coocurrence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unigram_count</th>\n",
       "      <th>word</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>I</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>There</th>\n",
       "      <th>enough</th>\n",
       "      <th>mind</th>\n",
       "      <th>No</th>\n",
       "      <th>A</th>\n",
       "      <th>Highbury</th>\n",
       "      <th>does</th>\n",
       "      <th>;--</th>\n",
       "      <th>happy</th>\n",
       "      <th>even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11454</td>\n",
       "      <td>,</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.164135</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.222368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.001484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6928</td>\n",
       "      <td>.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141022</td>\n",
       "      <td>0.007794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5183</td>\n",
       "      <td>to</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.065406</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.068686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4844</td>\n",
       "      <td>the</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4672</td>\n",
       "      <td>and</td>\n",
       "      <td>0.030822</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.084974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.001712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unigram_count word         ,         .        to       the       and  \\\n",
       "0          11454    ,  0.000000  0.000000  0.025231  0.033787  0.164135   \n",
       "1           6928    .  0.000000  0.002887  0.000000  0.000000  0.007506   \n",
       "2           5183   to  0.005016  0.004052  0.001929  0.065406  0.000772   \n",
       "3           4844  the  0.000000  0.000413  0.001858  0.000206  0.000000   \n",
       "4           4672  and  0.030822  0.007063  0.014127  0.080479  0.001284   \n",
       "\n",
       "         of         I         a    ...        There    enough      mind  \\\n",
       "0  0.009604  0.049764  0.222368    ...     0.000000  0.000087  0.000000   \n",
       "1  0.000000  0.141022  0.007794    ...     0.011547  0.000000  0.000000   \n",
       "2  0.000965  0.003087  0.068686    ...     0.000000  0.000000  0.000386   \n",
       "3  0.002064  0.000413  0.036953    ...     0.000000  0.000000  0.000826   \n",
       "4  0.008990  0.035317  0.084974    ...     0.000000  0.000000  0.000856   \n",
       "\n",
       "         No         A  Highbury      does       ;--     happy      even  \n",
       "0  0.000000  0.000262  0.000175  0.000437  0.000000  0.000175  0.001484  \n",
       "1  0.011980  0.042148  0.000289  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.001158  0.004052  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.003097  0.000826  0.000000  0.000000  0.000413  0.009703  \n",
       "4  0.000214  0.000428  0.001070  0.000428  0.000214  0.001712  0.001712  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we have to divide each bi-gram count by the count of the unigram to produce the probabilities\n",
    "def normalize_row(row):\n",
    "    for _word in unique_words: \n",
    "        row[_word] /= row['unigram_count']\n",
    "    return row\n",
    "\n",
    "bigrams_probabilites = coocurrence_df.apply(lambda x: normalize_row(x), axis=1)\n",
    "bigrams_probabilites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she was 0.0029350104821802936\n",
      "was to 0.00019293845263360988\n",
      "to the 0.0018579686209744012\n",
      "1.0521237465023985e-09\n"
     ]
    }
   ],
   "source": [
    "# FInally copute probability for some sentence\n",
    "sentence = ['she', 'was', 'to', 'the']\n",
    "final_probability = 1\n",
    "\n",
    "for _w1, _w2 in zip(sentence[:-1], sentence[1:]):\n",
    "    prob = bigrams_probabilites[bigrams_probabilites['word'] == _w2][_w1]\n",
    "    if len(prob)>0:\n",
    "        final_probability *= prob.iloc[0]\n",
    "        print(_w1, _w2, prob.iloc[0])\n",
    "    else:\n",
    "        print(_w1, _w2, 'Not Found')\n",
    "    \n",
    "print(final_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #1\n",
    "Multiplying individual bigram probabilities we end up with very __small numbers__ and there is a risk of __underflow__. Solution:\n",
    "\n",
    "__log(p1\\*p2\\*p3\\*p4) = log(p1) + log(p2) + log =(p3) + log(p4)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004985917555788305\n"
     ]
    }
   ],
   "source": [
    "final_probability = 0\n",
    "\n",
    "for _w1, _w2 in zip(sentence[:-1], sentence[1:]):\n",
    "    prob = bigrams_probabilites[bigrams_probabilites['word'] == _w2][_w1]\n",
    "    if len(prob)>0:\n",
    "        final_probability += prob.iloc[0]\n",
    "    \n",
    "print(final_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #2 : We still have to obtain an enourmous corpus in order to prevent having zero counts for some possible bigrams!\n",
    "- __Google N-grams__:  number of __bigrams: 314,843,401__, number of __tokens: 1,024,908,267,229__ - 6% of all volumes published in English\n",
    "- __Shakespeare__: __884,647 words, 29,066 tokens__. Produced 300,000 bigram types, out of __844 million possible bigrams__ - small percentage of the bigrams were ever seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of a language models\n",
    "- __Extrinsic__ - with respect to some __task__ - e.g. how well it predicted which is the correct spelling of a word in a spell-checking task.\n",
    "- __Intrinsic__ - estimate how well the model increases the probability for sentences from a __test corpus__. (! Should have same distribution of words in both sets). \n",
    "    - __Shanon game__ - how well will we __predict the next word__, given some context?\n",
    "    - __Perplexity__: \n",
    "$ PP(W) = P(w_1, w_2, ..., w_n)^{-\\frac{1}{n}} $\n",
    "    - For __uniform distribution__, perplexity equals the size of vocabulary:\n",
    "        - probability for generating 0, 1, 2, 3, 4, 5?\n",
    "    - __Minimizing perplexity__ is like maximizing probability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Compute the __perplexity__ with the same __training data__:  <br>\n",
    "[s] I am Sam [/s] <br>\n",
    "[s] Sam I am [/s] <br>\n",
    "[s] Sam I like [/s] <br>\n",
    "[s] Sam I do like [/s] <br>\n",
    "[s] do I like Sam [/s] <br>\n",
    "\n",
    "for the test __sentence__ : <br>\n",
    "[s] I do like Sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing Techniques\n",
    "- Generally, we want to avoid sparse statistics and to decide what to do in case we have a bi-gram count = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unigram_count</th>\n",
       "      <th>word</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>to</th>\n",
       "      <th>the</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>I</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>There</th>\n",
       "      <th>enough</th>\n",
       "      <th>mind</th>\n",
       "      <th>No</th>\n",
       "      <th>A</th>\n",
       "      <th>Highbury</th>\n",
       "      <th>does</th>\n",
       "      <th>;--</th>\n",
       "      <th>happy</th>\n",
       "      <th>even</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11454</td>\n",
       "      <td>,</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025231</td>\n",
       "      <td>0.033787</td>\n",
       "      <td>0.164135</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.222368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.001484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6928</td>\n",
       "      <td>.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.141022</td>\n",
       "      <td>0.007794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011980</td>\n",
       "      <td>0.042148</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5183</td>\n",
       "      <td>to</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.065406</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.068686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.004052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4844</td>\n",
       "      <td>the</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.036953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.009703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4672</td>\n",
       "      <td>and</td>\n",
       "      <td>0.030822</td>\n",
       "      <td>0.007063</td>\n",
       "      <td>0.014127</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.035317</td>\n",
       "      <td>0.084974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.001712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unigram_count word         ,         .        to       the       and  \\\n",
       "0          11454    ,  0.000000  0.000000  0.025231  0.033787  0.164135   \n",
       "1           6928    .  0.000000  0.002887  0.000000  0.000000  0.007506   \n",
       "2           5183   to  0.005016  0.004052  0.001929  0.065406  0.000772   \n",
       "3           4844  the  0.000000  0.000413  0.001858  0.000206  0.000000   \n",
       "4           4672  and  0.030822  0.007063  0.014127  0.080479  0.001284   \n",
       "\n",
       "         of         I         a    ...        There    enough      mind  \\\n",
       "0  0.009604  0.049764  0.222368    ...     0.000000  0.000087  0.000000   \n",
       "1  0.000000  0.141022  0.007794    ...     0.011547  0.000000  0.000000   \n",
       "2  0.000965  0.003087  0.068686    ...     0.000000  0.000000  0.000386   \n",
       "3  0.002064  0.000413  0.036953    ...     0.000000  0.000000  0.000826   \n",
       "4  0.008990  0.035317  0.084974    ...     0.000000  0.000000  0.000856   \n",
       "\n",
       "         No         A  Highbury      does       ;--     happy      even  \n",
       "0  0.000000  0.000262  0.000175  0.000437  0.000000  0.000175  0.001484  \n",
       "1  0.011980  0.042148  0.000289  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.001158  0.004052  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.003097  0.000826  0.000000  0.000000  0.000413  0.009703  \n",
       "4  0.000214  0.000428  0.001070  0.000428  0.000214  0.001712  0.001712  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_probabilites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add-one/Laplace Smoothing\n",
    "- Pretend __we saw each word one more time than we did__!\n",
    "$ P (w_i|w_{i-1}) = \\frac{c(w_{i-1},w_i) + 1}{c(w_i)+|V|} $\n",
    "- This\t__moves the probability mass__ from ‘the rich’ towards ‘the poor’\n",
    "- Why is it __bad__? \n",
    "    - Sometimes ~90% of the probability mass is spread across unseen events.\n",
    "    - We should know V beforehand!\n",
    "\n",
    "## Add-b Smoothing\n",
    "$ P (w_i|w_{i-1}) = \\frac{c(w_{i-1},w_i) + \\beta}{c(w_i)+\\beta|V|} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Take again the same __training data__. This time, we use a __bigram LM with Laplace smoothing__:\n",
    "\n",
    "[s] I am Sam [/s] <br>\n",
    "[s] Sam I am [/s] <br>\n",
    "[s] Sam I like [/s] <br>\n",
    "[s] Sam I do like [/s] <br>\n",
    "[s] do I like Sam [/s] <br>\n",
    "\n",
    "1. Give the following __bigram probabilities__ estimated by this model: <br>\n",
    "P(do|[s]) P(do|Sam) P(Sam|[s]) P(Sam|do) <br>\n",
    "P(I|Sam) P(I|do) P(like|I) <br>\n",
    "\n",
    "2. Calculate the __probabilities of the following sequences__ according to this model: <br>\n",
    "(8) [s] do Sam I like<br>\n",
    "(9) [s] Sam do I like<br>\n",
    "Which of the two sequences is more probable according to our LM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Turing\n",
    "\n",
    "### Intuition:\n",
    "You are fishing and caught: __10 carp, 3 perch, 2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish__ <br>\n",
    "- How likely is it that __next species is trout__? - 1/18\n",
    "- How likely is it that __next species is new__ (i.e. catfish or bass)?\n",
    "- Let’s use our estimate of __things, we saw once to estimate the new things__. - 3/18 (because N_1=3) \n",
    "- Now, how likely is it that next species is trout?  - Must be less than 1/18  How to estimate?  \n",
    " \n",
    "### Good-Turing Computation:\n",
    "- Define __N_c__ as\tthe __number of N-grams that occur c times__.\n",
    "- P([unseen]) = N1/N\n",
    "- Re-estimate counts of words: <br>\n",
    "$ c* = \\frac{(c+1)N_{c+1}}{N_c}$\n",
    "\n",
    "### Example:\n",
    "[s] I am Sam [/s] <br>\n",
    "[s] Sam I am [/s] <br>\n",
    "[s] Sam I like [/s] <br>\n",
    "[s] Sam I do like [/s] <br>\n",
    "\n",
    "- Unseen: 1/5 <br>\n",
    "- Seen once: c\\*(do) = (1+1)\\*2/5 = 4/5 <br>\n",
    "\n",
    "### Problems\n",
    "- When word occurs c times, but __no word occurs c+1 times__;\n",
    "- What happens when c(amazing|weather)=0 and c(amazing|trout)=0, and we __smooth unseen bigrams__? These will appear equal, but we would expect the __second to be more likely as weather a more frequent word__. Then, combine uni-gram, bi-gram and tri-gram models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff and Interpolation \n",
    "- Backoff: use trigram if you have a good evidence, otherwise - bigram, otherwise - unigram!\n",
    "- Interpola1on: mix unigram, bigram, trigram\n",
    "\n",
    "### Simple approach:\n",
    "$P(w_i|w_{i-1}, w_{i-2}) = \\lambda_1 P(w_i|w_{i-1},w_{i-2}) + \\lambda_2 P(w_i|w_{i-1}) + \\lambda_3 P(w_i) $ <br>\n",
    "$ \\lambda_1 + \\lambda_2 + \\lambda_3 = 1 $\n",
    "- Estimate lambdas on test set!\n",
    "- Can have $\\lambda$ depending on the context. For example, Witten-Bell Smoothing: <br>\n",
    "$ \\lambda_{w_{i-1}} = \\frac{u(w_{i-1})}{u(w_{i-1})+c(w_{i-1})} $ <br>\n",
    "where :\n",
    "    - u() - unique counts, c() - raw counts\n",
    "    - Then: \n",
    "$P(w_i|w_{i-1}) = \\lambda_{w_{i-1}} P(w_i|w_{i-1},w_{i-2}) + (1-\\lambda_{w_{i-1}}) P(w_i) $ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: \n",
    "1. __Train an uni-gram, bi-gram and tri-gram models__ on some subset of documents from nltk: (see what's available here: https://www.nltk.org/book/ch02.html)\n",
    "2. __Test__ the models, computing their __perplexity__, on some data from __same and from a different distribution__.\n",
    "3. Add and compare __add-one and backoff-and-interpolation for smoothing__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
