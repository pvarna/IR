{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__“If you cannot measure it, you cannot improve it”__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline - having a labeled dataset and a search models at hand, evaluate performance of the models (how well was the user's information request met?)\n",
    "\n",
    "\n",
    "Here we __estimate the performance with respect to each particular query__. In classification we will just have a list of results and will compute the metrics just once, not for each query!\n",
    "\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold: [1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Pred: [1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "doc_ids = list(range(1, 31))\n",
    "gold = [[1, 2, 6, 18, 20, 16, 24, 17, 3], \n",
    "        [9, 12, 4, 5, 1, 10, 22, 23, 25, 27, 11]]\n",
    "query_results = [[1, 5, 7, 15, 20, 2, 30, 21, 24, 4, 6, 18], \n",
    "                 [3, 9, 11, 12, 30, 7, 8, 4, 21, 22]]\n",
    "\n",
    "def to_vector(doc_ids, doc_set):\n",
    "    return [1 if doc in doc_set else 0 for doc in doc_ids]\n",
    "\n",
    "gold_v = [to_vector(doc_ids, _v) for _v in gold]\n",
    "predicted_v = [to_vector(doc_ids, _v) for _v in query_results]\n",
    "\n",
    "print(\"Gold:\", gold_v[0])\n",
    "print(\"Pred:\", predicted_v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 in the vector of gold annotations means that the document at this position is relevant to the query.<br>\n",
    "1 in the vector of predictions means that our model considers the document at this position as relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix:\n",
    "<img src=\"img/cm.png\" width=\"400\">\n",
    "\n",
    "__Diagonals is what we want to improve/increase!__\n",
    "\n",
    "__Exercise__: Calculate the CF for the queries:<br>\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0:\n",
      "[[15  6]\n",
      " [ 3  6]]\n",
      "Query 1:\n",
      "[[14  5]\n",
      " [ 6  5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Query {}:\".format(i))\n",
    "    print(confusion_matrix(gold_v[i], predicted_v[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy - how many of the retrieved results are correct?\n",
    "\n",
    "$$\\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "__Exercise__ : Calculate the accuracy for the two queries<br>\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0:\n",
      "0.7\n",
      "Query 1:\n",
      "0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Query {}:\".format(i))\n",
    "    print(accuracy_score(gold_v[i], predicted_v[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993\n",
      "0.995\n"
     ]
    }
   ],
   "source": [
    "# Why don't we just use accuracy?!\n",
    "test_dog_ids = list(range(1000))\n",
    "some_gold = [1, 2, 3, 4, 5]\n",
    "print(accuracy_score(to_vector(test_dog_ids, some_gold), to_vector(test_dog_ids, [1, 4, 6, 10, 11, 7])))\n",
    "print(accuracy_score(to_vector(test_dog_ids, some_gold), to_vector(test_dog_ids, [])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In real IR systems we have __thousands of documents__. Thus, accuracy will cause our IR method to return __NO RESULTS__.\n",
    "- The change __improvements__ in the IR method are __less visible__ - we already have an accuracy of 0.995!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "<img src=\"img/cm_pr.png\" width=\"400\">\n",
    "\n",
    "Precision : How many of the retrieved results were useful? <br>\n",
    "$$Precision = \\frac{TP}{TP + FP}$$<br>  \n",
    "Recall : Were there any useful pages left not retrieved?  <br>\n",
    "$$Recall = \\frac{TP}{TP + FN}$$ \n",
    "\n",
    "__Exercise:__ \n",
    "- Compute Precision and Recall for the queries:<br>\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11 <br>\n",
    "\n",
    "- What does it mean to have a precision = 1/ recall = 1 ?\n",
    "- When does precision decrease? When does recall decrease?\n",
    "- When do we get the highest recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0:\n",
      "Precision: 0.5\n",
      "Recall: 0.6666666666666666\n",
      "Query 1:\n",
      "Precision: 0.5\n",
      "Recall: 0.45454545454545453\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Query {}:\".format(i))\n",
    "    print(\"Precision:\", precision_score(gold_v[i], predicted_v[i]))\n",
    "    print(\"Recall:\", recall_score(gold_v[i], predicted_v[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which of the metrics is more important? - Usually depends on the business requirements.\n",
    "- We can also __combine Precision and Recall__, using their harmonic mean! \n",
    "\n",
    "## F1 score\n",
    "$$ F_{\\beta} = \\frac{(\\beta^{2}+1)PR}{\\beta^{2}P+R} $$\n",
    "\n",
    "Most common F1:\n",
    "$$ F_{1} = \\frac{2PR}{P+R} $$\n",
    "\n",
    "\n",
    "__Exercise__: \n",
    "- Compute F1 for the queries.<br>\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11\n",
    "\n",
    "- Does the order of the results made the user’s search for information easier or harder? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0:\n",
      "F1: 0.5714285714285715\n",
      "Query 1:\n",
      "F1: 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for i in range(2):\n",
    "    print(\"Query {}:\".format(i))\n",
    "    print(\"F1:\", f1_score(gold_v[i], predicted_v[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics with account for position\n",
    "- So far : we didn't account for the __position of the correct relevant documents__ in the returned list of documents!\n",
    "- Example: If your __first page contains 20 items__, you might want to __optimize P@20, R@20__.\n",
    "\n",
    "__Exercise__: Compute P@N, R@N for N= 1, 2, 3, 4, 5<br>\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 0:\n",
      "N: 1, Precision: 1.0, Recall: 0.1111111111111111\n",
      "N: 2, Precision: 0.5, Recall: 0.1111111111111111\n",
      "N: 3, Precision: 0.3333333333333333, Recall: 0.1111111111111111\n",
      "N: 4, Precision: 0.25, Recall: 0.1111111111111111\n",
      "N: 5, Precision: 0.4, Recall: 0.2222222222222222\n",
      "N: 6, Precision: 0.5, Recall: 0.3333333333333333\n",
      "N: 7, Precision: 0.42857142857142855, Recall: 0.3333333333333333\n",
      "Query 1:\n",
      "N: 1, Precision: 0.0, Recall: 0.0\n",
      "N: 2, Precision: 0.5, Recall: 0.09090909090909091\n",
      "N: 3, Precision: 0.6666666666666666, Recall: 0.18181818181818182\n",
      "N: 4, Precision: 0.75, Recall: 0.2727272727272727\n",
      "N: 5, Precision: 0.6, Recall: 0.2727272727272727\n",
      "N: 6, Precision: 0.5, Recall: 0.2727272727272727\n",
      "N: 7, Precision: 0.42857142857142855, Recall: 0.2727272727272727\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"Query {}:\".format(i))\n",
    "    for n in range(1, 8):\n",
    "        print(\"N:\", n, end=', ')\n",
    "        print(\"Precision:\", precision_score(gold_v[i], to_vector(doc_ids, query_results[i][:n])), end=', ')\n",
    "        print(\"Recall:\", recall_score(gold_v[i], to_vector(doc_ids, query_results[i][:n])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve: compute the precision achieved taking just top N of the returned results, i.e. precision at different recall levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'signature' from 'sklearn.utils.fixes' (C:\\Users\\didimitrov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_curve\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'signature' from 'sklearn.utils.fixes' (C:\\Users\\didimitrov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(\"Query {}:\".format(i))\n",
    "    rankings = [query_results[i].index(j)/len(doc_ids) if j in query_results[i] else 0 for j in doc_ids]\n",
    "    precision, recall, _ = precision_recall_curve(gold_v[i], rankings)\n",
    "    \n",
    "    step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.02])\n",
    "    plt.xlim([0.0, 1.02])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More metrics:\n",
    "- __R-Precision__ - having 20 relevant documents for a query, measure precision at 20, R=20. Dynamic metric, which depends on the number of relevant documents for each query.\n",
    "    - A perfect system can score 1.0 over all queries.\n",
    "- __Mean Reciprocal Rank__\n",
    "\n",
    " $$\\text{MRR} = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|}\\sum_{j=1}^{|RelD_{Q}|} \\frac{1}{\\text{rank}_{i,j}}. $$\n",
    "    - Usually computed only for the first relevant document!\n",
    "    - Reciprocal Rank for the first correct document - returned __at position 1 = 1, at position 2 = 1/2,__ etc.<br>\n",
    "    - The reciprocal rank is low if a relevant document is returned at the end of the list with results.\n",
    "- __Mean Average Precision__ - average of the precision value obtained for the top k documents, each time a relevant doc is retrieved\n",
    "     - Compute __MEAN over queries__\n",
    "     - For each query, compute: __AVERAGE of the Precision@N__, computed at each relevant document\n",
    "$$ AvgP = \\frac{\\sum_{N=1}^{|D_{Q}|} (P@N \\times rel@N)}{RelD_{Q}} $$\n",
    "\n",
    "- Even more: __discounted cumulative gain__ (DCG) - when relevance is not binary, but we may measure a degree of relevance - e.g. exact match, relevant, not relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise:__ Compute MRR, MAP and R-precision for: <br>\n",
    "Q1 : __D1__, D5, D7, D15, __D20__, __D2__, D30, D21, __D24__, D4, __D6__, __D18__ | GOLD: D1, D2, D6, D18, D20, D16, D24, D17, 3<br>\n",
    "Q2 : D3, __D9__, __D11__, __D12__, D30, D7, D8, D4, D21, __D22__ | GOLD: D9, D12, D4, D5, D1, D10, D22, D23, D25, D27, D11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to navigate your efforts depending on the metrics?\n",
    "- Use the selected metric to improve the performance on the validation set.\n",
    "- Once tuned, estimate the performance on the test set, too. \n",
    "- The train, validation and test scores w.r.t the selected metric should help you to estimate bias/variance.\n",
    "- Then, it would be easier to decide whether to get more data/make more complex/simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online metrics - see how users interact with your system\n",
    "\n",
    "- Session abandonment rate and session success rate\n",
    "- Click-through rate (CTR) \n",
    "- Satisfaction of a click - how much time is spent on a URL\n",
    "- Time before clicking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available Datasets for Experiments with evaluation metrics:\n",
    "(only for classification) <br>\n",
    "https://www.nltk.org/book/ch02.html <br>\n",
    "https://archive.ics.uci.edu/ml/datasets.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
