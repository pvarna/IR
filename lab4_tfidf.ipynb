{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the List of Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When searching we receive a huge list of results, then we have to rank the results and return the most informative!\n",
    "\n",
    "## Number of overlapping words:\n",
    "- not normalized by length of document\n",
    "\n",
    "## Jaccard Coefficient\n",
    "- $ |\\space X \\cap Y \\space|\\space  /\\space  |\\space X \\cup Y \\space | $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\didimitrov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categories:  ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "Housing articles: ['test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/11170', 'training/11665', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/9615']\n",
      "\n",
      "Words in an arbitrary article: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "# NLTK supports access to different datasets https://www.nltk.org/book/ch02.html\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "print(\"\\nCategories: \", reuters.categories())\n",
    "\n",
    "housing_articles = reuters.fileids('housing')\n",
    "print(\"\\nHousing articles:\", housing_articles)\n",
    "\n",
    "print(\"\\nWords in an arbitrary article:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "Long Document words number: 131\n",
      "Long Document overlap: 3\n",
      "Long Document JC: 0.03571428571428571\n",
      "\n",
      "Short Document words number: 7\n",
      "Short Document overlap: 3\n",
      "Short Document JC: 0.375\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def word_overlap(doc_tokens, query_tokens):\n",
    "    return sum([1 for _tok in query_tokens if _tok in doc_tokens])\n",
    "\n",
    "def jaccard_coeff(doc_tokens, query_tokens):\n",
    "    # naive intersection of sets\n",
    "    return len(set(doc_tokens).intersection(set(query_tokens))) / len(set(doc_tokens).union(set(query_tokens)))\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"Long Document words number:\", len(reuters.words('training/6067')))\n",
    "print(\"Long Document overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Long Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document words number:\", len(short_similar_document))\n",
    "print(\"Short Document overlap:\", word_overlap(query_tokens, short_similar_document))\n",
    "print(\"Short Document JC:\", jaccard_coeff(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__But we also want to__:\n",
    "- Give _more weight_ to _less frequent words_ in the documents - __Balridge, prices__\n",
    "- Give _less weight_ to _more frequent words_ in the documents - __how, much, housing, to__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document word overlap: 13\n",
      "Document JC: 0.06593406593406594\n",
      "Document Content: ['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', 'HOUSING', 'GROWTH', 'Commerce']\n"
     ]
    }
   ],
   "source": [
    "query_tokens = tokenizer.tokenize('how much will the housing go up in the next month according to Balridge?')\n",
    "print(\"Document word overlap:\", word_overlap(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document JC:\", jaccard_coeff(query_tokens, reuters.words('training/6067')))\n",
    "print(\"Document Content:\", reuters.words('training/6067')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - Term Frequency- Inverted Document Frequency\n",
    "- View documents as __Bags Of Words__\n",
    "- Mary lent John some money. = John lent Mary some money.\n",
    "- Formula: \n",
    "\n",
    "$$TF * IDF (word, document) = (1+log(tf(word, document)) * log(\\frac{n}{df(word)})$$\n",
    "- n - total number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "- __Frequency of word in a document (here, raw count)__\n",
    "- __0 if the term is not met in the document!!!__\n",
    "- Relevance does not increase proportionally with frequency -> __log (base of 10)__\n",
    "- Makes TF-IDF __increase with the number of occurrences__ within a doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>34</td>\n",
       "      <td>2.531479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>31</td>\n",
       "      <td>2.491362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>21</td>\n",
       "      <td>2.322219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pct</td>\n",
       "      <td>15</td>\n",
       "      <td>2.176091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.113943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000</td>\n",
       "      <td>13</td>\n",
       "      <td>2.113943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>September</td>\n",
       "      <td>11</td>\n",
       "      <td>2.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>units</td>\n",
       "      <td>11</td>\n",
       "      <td>2.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>starts</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>August</td>\n",
       "      <td>10</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  freq        tf\n",
       "0  .          34    2.531479\n",
       "1  ,          31    2.491362\n",
       "2  in         21    2.322219\n",
       "3  pct        15    2.176091\n",
       "4  1          13    2.113943\n",
       "5  000        13    2.113943\n",
       "6  September  11    2.041393\n",
       "7  units      11    2.041393\n",
       "8  starts     10    2.000000\n",
       "9  August     10    2.000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>freq</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>689</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>below</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>level</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>687</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     token  freq   tf\n",
       "110  689    1     1.0\n",
       "111  11     1     1.0\n",
       "112  below  1     1.0\n",
       "113  level  1     1.0\n",
       "114  687    1     1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip3 install pandas\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(Counter(reuters.words('test/20116')).most_common(), columns=['token', 'freq'])\n",
    "df['tf'] = 1 + np.log10(df['freq'])\n",
    "df.head(10)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency\n",
    "- __Number of documents containing the word__ - an inversed measure of significance\n",
    "- Logarithm with base 10 dampens the effect of IDF\n",
    "- Affects ranking of queries with __at least 2 terms__\n",
    "- Makes TFIDF __increase with the rarity of the term in the collection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>.</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>,</td>\n",
       "      <td>19</td>\n",
       "      <td>0.022276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>the</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>to</td>\n",
       "      <td>17</td>\n",
       "      <td>0.070581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>S</td>\n",
       "      <td>16</td>\n",
       "      <td>0.096910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  doc_freq       idf\n",
       "70     .        20  0.000000\n",
       "186    ,        19  0.022276\n",
       "110  the        17  0.070581\n",
       "173   to        17  0.070581\n",
       "106    S        16  0.096910"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>501</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>fallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  doc_freq      idf\n",
       "398     501         1  1.30103\n",
       "399  fallen         1  1.30103\n",
       "405     500         1  1.30103\n",
       "369     990         1  1.30103\n",
       "832     241         1  1.30103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "document_frequency = defaultdict(lambda: 0)\n",
    "for fileid in housing_articles:\n",
    "    for _word in set(reuters.words(fileid)):\n",
    "        document_frequency[_word] += 1\n",
    "\n",
    "idf_df = pd.DataFrame(list(document_frequency.items()), columns=['word', 'doc_freq'])\n",
    "idf_df['idf'] = np.log10(len(housing_articles)/idf_df['doc_freq'])\n",
    "idf_df.sort_values(by=['idf'], inplace=True)\n",
    "idf_df.head()\n",
    "idf_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we estimate score for a document D w.r.t. a query Q, __summing over tfidf scores of the words in both D and Q (intersection)__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/6067')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  ['housing', 'growth', 'next', 'month']\n",
      "\n",
      "Long Document TF-IDF: 1.2596373105057561\n",
      "\n",
      "Short Document TF-IDF: 2.1627272974976997\n"
     ]
    }
   ],
   "source": [
    "def tfidf_score(query_tokens, document_tokens):\n",
    "    # naive implementation\n",
    "    def tfidf(word):\n",
    "        #TODO: Put some comments around idf_df as a prerequisite!!!\n",
    "        #How do we get values for the variable idf_df ? :: Recompute them for the new documents.\n",
    "        return (1 + np.log10(document_tokens.count(word))) * idf_df[idf_df['word']==word].iloc[0]['idf']\n",
    "    \n",
    "    overlapping_tokens = set(query_tokens).intersection(set(document_tokens))\n",
    "    return sum([tfidf(_word) for _word in overlapping_tokens])\n",
    "\n",
    "query_tokens = tokenizer.tokenize('housing growth next month')\n",
    "print(\"Query: \", query_tokens)\n",
    "print(\"\\nLong Document TF-IDF:\", tfidf_score(query_tokens, reuters.words('training/6067')))\n",
    "\n",
    "short_similar_document = tokenizer.tokenize('Baldrige predicts housing growth next week.')\n",
    "print(\"\\nShort Document TF-IDF:\", tfidf_score(query_tokens, short_similar_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BALDRIGE', 'PREDICTS', 'SOLID', 'U', '.', 'S', '.', ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.words('training/6067')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Now compute the TF-IDF score of the query to the documents:\n",
    "- Query: 'Who was the first man ever to swim around Britain?'\n",
    "- Doc1: 'Ross Edgley, at 33 - first man to swim around Britain'\n",
    "- Doc2: 'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea'\n",
    "- Doc3: 'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?'\n",
    "- Doc4: 'Welcome to the world of strongman swimming | British GQ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!TODO: Bring the solution back into the notebook!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space\n",
    "- Each document can be represented by a vector, where the terms are the axes of the space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    'Ross Edgley, at 33 - first man to swim around Britain',\n",
    "    'Ross Edgley to Circumnavigate Britain Spent 5 Months at Sea',\n",
    "    'Get Set 4 Swimming - H2OMG! Can this man swim around Britain?',\n",
    "    'Welcome to the world of strongman swimming | British GQ'\n",
    "]\n",
    "query = 'Who was the first man ever to swim around Britain?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.feature_extraction.text:\n",
    "- __CountVectorizer__ - Convert a collection of text documents to a matrix of token counts.\n",
    "- __TfidfVectorizer__ - Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "- Two main methods __fit__ and __transform__ :\n",
    "    - __fit__ goes through the provided documents and __collects the vocabulary__\n",
    "    - __transform__ transforms __documents in text representation to a vector representation__ according to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\didimitrov\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.0.post1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ross': 15, 'edgley': 7, 'at': 2, '33': 0, 'first': 8, 'man': 12, 'to': 24, 'swim': 20, 'around': 1, 'britain': 3, 'circumnavigate': 6, 'spent': 18, 'months': 13, 'sea': 16, 'get': 9, 'set': 17, 'swimming': 21, 'h2omg': 11, 'can': 5, 'this': 23, 'welcome': 25, 'the': 22, 'world': 26, 'of': 14, 'strongman': 19, 'british': 4, 'gq': 10}\n"
     ]
    }
   ],
   "source": [
    "#! pip install sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(documents)\n",
    "print(count_vectorizer.vocabulary_) # word to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform produces a sparse representations of documents - only values != 0\n",
    "# we need toarray() to preview the whole lists\n",
    "count_vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39090017 0.30819018 0.30819018 0.24950651 0.         0.\n",
      "  0.         0.30819018 0.39090017 0.         0.         0.\n",
      "  0.30819018 0.         0.         0.30819018 0.         0.\n",
      "  0.         0.         0.30819018 0.         0.         0.\n",
      "  0.24950651 0.         0.        ]\n",
      " [0.         0.         0.30505473 0.24696809 0.         0.\n",
      "  0.38692324 0.30505473 0.         0.         0.         0.\n",
      "  0.         0.38692324 0.         0.30505473 0.38692324 0.\n",
      "  0.38692324 0.         0.         0.         0.         0.\n",
      "  0.24696809 0.         0.        ]\n",
      " [0.         0.28061469 0.         0.22718178 0.         0.35592415\n",
      "  0.         0.         0.         0.35592415 0.         0.35592415\n",
      "  0.28061469 0.         0.         0.         0.         0.35592415\n",
      "  0.         0.         0.28061469 0.28061469 0.         0.35592415\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.35291425 0.\n",
      "  0.         0.         0.         0.         0.35291425 0.\n",
      "  0.         0.         0.35291425 0.         0.         0.\n",
      "  0.         0.35291425 0.         0.27824164 0.35291425 0.\n",
      "  0.22526059 0.35291425 0.35291425]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ross': 15,\n",
       " 'edgley': 7,\n",
       " 'at': 2,\n",
       " '33': 0,\n",
       " 'first': 8,\n",
       " 'man': 12,\n",
       " 'to': 24,\n",
       " 'swim': 20,\n",
       " 'around': 1,\n",
       " 'britain': 3,\n",
       " 'circumnavigate': 6,\n",
       " 'spent': 18,\n",
       " 'months': 13,\n",
       " 'sea': 16,\n",
       " 'get': 9,\n",
       " 'set': 17,\n",
       " 'swimming': 21,\n",
       " 'h2omg': 11,\n",
       " 'can': 5,\n",
       " 'this': 23,\n",
       " 'welcome': 25,\n",
       " 'the': 22,\n",
       " 'world': 26,\n",
       " 'of': 14,\n",
       " 'strongman': 19,\n",
       " 'british': 4,\n",
       " 'gq': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "print(tfidf_vectorizer.fit_transform(documents).toarray())\n",
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing vector similarities\n",
    "- We would like to find documents close to a given document or the closest documents to a query\n",
    "- __Euclidean distance__? - shorter documents will be closer to each other rather than documents talking about same topic\n",
    "- __Cosine similarity__ of the angle between two documents\n",
    "    - divide each vector by its norm to achieve __unit length vectors__\n",
    "    - cosine similarity is simply the __dot product__ of two unit length vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cosine SImilarity](img/cosine.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = np.array([1, 0, 0, 1, 2])\n",
    "vector2 = np.array([0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40824829, 0.        , 0.        , 0.40824829, 0.81649658]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.40824829, 0.        , 0.        , 0.40824829, 0.81649658])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "preprocessing.normalize([vector1], norm='l2')\n",
    "vector1 / np.sqrt(sum(vector1**2))\n",
    "\n",
    "unit_vector1 = preprocessing.normalize([vector1], norm='l2')[0]\n",
    "unit_vector2 = preprocessing.normalize([vector2], norm='l2')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7071067811865477"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(unit_vector1, unit_vector2)\n",
    "sum([unit_vector1[i]*unit_vector2[i] for i in range(len(unit_vector1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.70710678],\n",
       "       [0.70710678, 1.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity([vector1, vector2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise : calculate the closes document to the query from the previous exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who was the first man ever to swim around Britain?\n",
      "1st Closest document: Get Set 4 Swimming - H2OMG! Can this man swim around Britain? Score: 0.8159745583466792\n",
      "2nd Closest document: Ross Edgley, at 33 - first man to swim around Britain Score: 0.7678877104085525\n"
     ]
    }
   ],
   "source": [
    "def get_closest_documents(query, vectorizer, train_corpus_vectors, top_n=2):\n",
    "    \"\"\"Vectorizer should be fit on the documents beforehand.\n",
    "        Returns tuples of (similarity, indexes) of closest documents\"\"\"\n",
    "   pass\n",
    "\n",
    "train_corpus_vectors = tfidf_vectorizer.transform(documents)\n",
    "closest_documents = get_closest_documents(query, tfidf_vectorizer, train_corpus_vectors)\n",
    "print('Query:', query)\n",
    "print('1st Closest document: {} Score: {}'.format(documents[closest_documents[0][1]], closest_documents[0][0]))\n",
    "print('2nd Closest document: {} Score: {}'.format(documents[closest_documents[1][1]], closest_documents[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: using the friends corpus try to create an IR chatbot:\n",
    "- User writes a sentences and we find the __closest sentence__ from the transcript\n",
    "- We need to take __the answer__ to that sentence to make a dialogue! \n",
    "- Provide the bot with a __personality__, selecting only the tuple cues, where the answer is by a specific person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\didimitrov\\AppData\\Local\\Temp\\ipykernel_19692\\454469593.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scene_id</th>\n",
       "      <th>person</th>\n",
       "      <th>gender</th>\n",
       "      <th>original_line</th>\n",
       "      <th>line</th>\n",
       "      <th>metadata</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>496501</td>\n",
       "      <td>266</td>\n",
       "      <td>RACHEL</td>\n",
       "      <td>F</td>\n",
       "      <td>Rachel: Let's just say my Curious George doll is no longer curious.</td>\n",
       "      <td>Let's just say my Curious George doll is no longer curious.</td>\n",
       "      <td>Let_VM21 's_VM22 just_RR say_VVI my_APPGE Curious_JJ George_NP1 doll_NN1 is_VBZ no_RR21 longer_RR22 curious_JJ ._.</td>\n",
       "      <td>0121.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>47001</td>\n",
       "      <td>23</td>\n",
       "      <td>ROSS</td>\n",
       "      <td>M</td>\n",
       "      <td>Ross: Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?</td>\n",
       "      <td>Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?</td>\n",
       "      <td>Well_RR uh_UH uh_UH I_PPIS1 do_VD0 n't_XX know_VVI okay_RR okay_RR how_RRQ about_II with_IW the_AT uh_UH with_IW the_AT baby_NN1 's_GE name_NN1 ?_?</td>\n",
       "      <td>0102.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28980</th>\n",
       "      <td>2894901</td>\n",
       "      <td>1489</td>\n",
       "      <td>ROSS</td>\n",
       "      <td>M</td>\n",
       "      <td>Ross: Hm-mmm.</td>\n",
       "      <td>Hmmmm.</td>\n",
       "      <td>Hm-mmm._NNU</td>\n",
       "      <td>0519.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22389</th>\n",
       "      <td>2235801</td>\n",
       "      <td>1147</td>\n",
       "      <td>RACHEL</td>\n",
       "      <td>F</td>\n",
       "      <td>Rachel: Relaxi-Taxi!</td>\n",
       "      <td>Relaxi-Taxi!</td>\n",
       "      <td>Relaxi-Taxi_NP1 !_!</td>\n",
       "      <td>0417.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59246</th>\n",
       "      <td>5921501</td>\n",
       "      <td>2968</td>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>F</td>\n",
       "      <td>Phoebe: Noooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.</td>\n",
       "      <td>Nooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.</td>\n",
       "      <td>Nooo_NN1 !_! Ok_RR maybe_RR if_CS we_PPIS2 just_RR break_VV0 it_PPH1 down_RP ._. Ok_RR let_VM21 's_VM22 try_VVI at_II one_MC1 syllable_NN1 at_II a_AT1 time_NNT1 ._. Ok_RR ?_? So_RR repeat_VV0 after_II me._NNU je_FW ._.</td>\n",
       "      <td>1013.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id scene_id  person gender  \\\n",
       "4974   496501   266      RACHEL  F       \n",
       "470    47001    23       ROSS    M       \n",
       "28980  2894901  1489     ROSS    M       \n",
       "22389  2235801  1147     RACHEL  F       \n",
       "59246  5921501  2968     PHOEBE  F       \n",
       "\n",
       "                                                                                                                 original_line  \\\n",
       "4974   Rachel: Let's just say my Curious George doll is no longer curious.                                                       \n",
       "470    Ross: Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?                               \n",
       "28980  Ross: Hm-mmm.                                                                                                             \n",
       "22389  Rachel: Relaxi-Taxi!                                                                                                      \n",
       "59246  Phoebe: Noooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.   \n",
       "\n",
       "                                                                                                                 line  \\\n",
       "4974   Let's just say my Curious George doll is no longer curious.                                                      \n",
       "470    Well, uh, uh, I don't know, okay, okay, how about with the, uh, with the baby's name?                            \n",
       "28980  Hmmmm.                                                                                                           \n",
       "22389  Relaxi-Taxi!                                                                                                     \n",
       "59246  Nooo! Ok, maybe if we just break it down. Ok, let's try at one syllable at a time. Ok? So repeat after me. je.   \n",
       "\n",
       "                                                                                                                                                                                                                         metadata  \\\n",
       "4974   Let_VM21 's_VM22 just_RR say_VVI my_APPGE Curious_JJ George_NP1 doll_NN1 is_VBZ no_RR21 longer_RR22 curious_JJ ._.                                                                                                           \n",
       "470    Well_RR uh_UH uh_UH I_PPIS1 do_VD0 n't_XX know_VVI okay_RR okay_RR how_RRQ about_II with_IW the_AT uh_UH with_IW the_AT baby_NN1 's_GE name_NN1 ?_?                                                                          \n",
       "28980  Hm-mmm._NNU                                                                                                                                                                                                                  \n",
       "22389  Relaxi-Taxi_NP1 !_!                                                                                                                                                                                                          \n",
       "59246  Nooo_NN1 !_! Ok_RR maybe_RR if_CS we_PPIS2 just_RR break_VV0 it_PPH1 down_RP ._. Ok_RR let_VM21 's_VM22 try_VVI at_II one_MC1 syllable_NN1 at_II a_AT1 time_NNT1 ._. Ok_RR ?_? So_RR repeat_VV0 after_II me._NNU je_FW ._.   \n",
       "\n",
       "       filename  \n",
       "4974   0121.txt  \n",
       "470    0102.txt  \n",
       "28980  0519.txt  \n",
       "22389  0417.txt  \n",
       "59246  1013.txt  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "friends_corpus = pd.read_csv(\"data/friends-final.txt\", sep='\\t')\n",
    "friends_corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>There's nothing to tell! He's just some guy I work with!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JOEY</td>\n",
       "      <td>C'mon, you're going out with the guy! There's gotta be something wrong with him!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Wait, does he eat chalk?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PHOEBE</td>\n",
       "      <td>Just, 'cause, I don't want her to go through what I went through with Carl- oh!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MONICA</td>\n",
       "      <td>Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Sounds like a date to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Oh, yeah. Had that dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>Then I look down, and I realize there's a phone... there.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     person  \\\n",
       "0  MONICA     \n",
       "1  JOEY       \n",
       "2  CHANDLER   \n",
       "3  PHOEBE     \n",
       "4  PHOEBE     \n",
       "5  MONICA     \n",
       "6  CHANDLER   \n",
       "7  CHANDLER   \n",
       "8  ALL        \n",
       "9  CHANDLER   \n",
       "\n",
       "                                                                                                                  line  \n",
       "0  There's nothing to tell! He's just some guy I work with!                                                             \n",
       "1  C'mon, you're going out with the guy! There's gotta be something wrong with him!                                     \n",
       "2  Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?                                               \n",
       "3  Wait, does he eat chalk?                                                                                             \n",
       "4  Just, 'cause, I don't want her to go through what I went through with Carl- oh!                                      \n",
       "5  Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.         \n",
       "6  Sounds like a date to me.                                                                                            \n",
       "7  Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.  \n",
       "8  Oh, yeah. Had that dream.                                                                                            \n",
       "9  Then I look down, and I realize there's a phone... there.                                                            "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example conversation\n",
    "friends_corpus[friends_corpus['scene_id']=='1'][['person', 'line']][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60849, 15032)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer().fit(friends_corpus.line)\n",
    "train_corpus = vectorizer.transform(friends_corpus.line)\n",
    "train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence:  C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
      "Its vector representation:  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "An id of a word from the sentence:  14756\n",
      "The word tf-idf score:  0.41270625402865396\n"
     ]
    }
   ],
   "source": [
    "print(\"First sentence: \", friends_corpus.line.values[1])\n",
    "print(\"Its vector representation: \", train_corpus[1].toarray())\n",
    "print(\"An id of a word from the sentence: \", vectorizer.vocabulary_['with'])\n",
    "print(\"The word tf-idf score: \", train_corpus[1].toarray()[0][vectorizer.vocabulary_['with']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8125, 5411)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the previous line, which the cue follows in the dialogue\n",
    "friends_corpus['previous_line'] = ['DUMMY PREVIOUS LINE'] + friends_corpus['line'].values[:-1].tolist()\n",
    "# select only the cues which are made by JOEY\n",
    "joey_line_tuples = friends_corpus[friends_corpus.person == 'JOEY']\n",
    "# create a vectorizer and training space of the documents in the vector space\n",
    "joey_vectorizer = TfidfVectorizer().fit(joey_line_tuples.previous_line)\n",
    "joey_train_corpus = joey_vectorizer.transform(joey_line_tuples.previous_line)\n",
    "joey_train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_utterance(cue, vectorizer,  train_corpus, top_n=5):\n",
    "   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,\n",
       "  \"Joey Tribbiani! From the wall! Okay, maybe this will jog your memory, huh? Huh? Okay eh-ah-anyway, I'm ready to go back up on the wall I'm the star of a new TV show.\"),\n",
       " (1.0,\n",
       "  \"Oh, hi, I'm Joey. My stupid friends are buying this house. Who are you?\"),\n",
       " (0.7527478944848099, 'Me.'),\n",
       " (0.7527478944848099, \"Alright. I'll give you one hint. Warren Beatty.\")]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_closest_utterance('who are you?', joey_vectorizer, joey_train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
